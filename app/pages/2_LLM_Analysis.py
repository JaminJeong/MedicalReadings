import os
import sys

# ì•± ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import streamlit as st

from core.dicom_loader import extract_pixel_array, get_window_defaults, load_xray
from core.image_processor import apply_windowing, array_to_png_bytes
from utils.prompt_templates import PROMPT_TEMPLATES

st.set_page_config(
    page_title="LLM Analysis - Medical Readings",
    page_icon="ğŸ¤–",
    layout="wide",
)

# ì„¸ì…˜ ìƒíƒœ ê¸°ë³¸ê°’
for key, val in {
    "current_image_bytes": None,
    "last_report": "",
}.items():
    if key not in st.session_state:
        st.session_state[key] = val


# â”€â”€ LLM ê°€ìš©ì„± í™•ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(ttl=30, show_spinner=False)
def check_availability() -> dict:
    avail = {}

    avail["GPT"] = bool(os.getenv("OPENAI_API_KEY"))
    avail["Gemini"] = bool(os.getenv("GOOGLE_API_KEY"))

    try:
        import torch  # noqa: F401
        import transformers  # noqa: F401
        avail["MedGemma"] = True
    except ImportError:
        avail["MedGemma"] = False

    try:
        import requests
        host = os.getenv("OLLAMA_HOST", "http://localhost:11434")
        resp = requests.get(f"{host}/api/tags", timeout=2)
        avail["Ollama"] = resp.status_code == 200
    except Exception:
        avail["Ollama"] = False

    return avail


# â”€â”€ ì‚¬ì´ë“œë°”: LLM ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.sidebar:
    st.header("LLM ì„¤ì •")

    availability = check_availability()

    # ìƒíƒœ í‘œì‹œ
    for name, avail in availability.items():
        icon = "ğŸŸ¢" if avail else "ğŸ”´"
        st.markdown(f"{icon} **{name}**")

    st.markdown("---")

    selected_llm = st.radio(
        "LLM ì„ íƒ",
        list(availability.keys()),
        key="selected_llm_radio",
    )

    if not availability[selected_llm]:
        st.warning(f"{selected_llm}ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\ní™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ì„œë¹„ìŠ¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    st.markdown("---")
    st.subheader("ëª¨ë¸ íŒŒë¼ë¯¸í„°")

    # LLMë³„ íŒŒë¼ë¯¸í„° UI
    if selected_llm == "GPT":
        gpt_model = st.selectbox("Model", ["gpt-4o", "gpt-4o-mini"])
        temperature = st.slider("Temperature", 0.0, 1.0, 0.3, 0.05)
        max_tokens = st.number_input("Max Tokens", 256, 4096, 2048, step=256)

    elif selected_llm == "Gemini":
        gemini_model = st.selectbox(
            "Model", ["gemini-1.5-pro", "gemini-1.5-flash", "gemini-2.0-flash"]
        )
        temperature = st.slider("Temperature", 0.0, 1.0, 0.3, 0.05)
        max_tokens = st.number_input("Max Tokens", 256, 4096, 2048, step=256)

    elif selected_llm == "MedGemma":
        medgemma_model = st.selectbox(
            "Model", ["google/medgemma-4b-it", "google/medgemma-27b-it"]
        )
        max_new_tokens = st.number_input("Max New Tokens", 128, 1024, 512, step=64)
        if not availability["MedGemma"]:
            st.info(
                "MedGemma ì‚¬ìš©ì„ ìœ„í•´:\n"
                "```\npip install torch transformers accelerate\n```"
            )

    elif selected_llm == "Ollama":
        ollama_model = "llava:13b"
        try:
            import requests as _req
            host = os.getenv("OLLAMA_HOST", "http://localhost:11434")
            resp = _req.get(f"{host}/api/tags", timeout=3)
            if resp.status_code == 200:
                model_list = [m["name"] for m in resp.json().get("models", [])]
                if model_list:
                    ollama_model = st.selectbox("Model", model_list)
                else:
                    st.info("ì„¤ì¹˜ëœ ëª¨ë¸ ì—†ìŒ\n`ollama pull llava:13b`")
                    ollama_model = st.text_input("Model name", "llava:13b")
            else:
                ollama_model = st.text_input("Model name", "llava:13b")
        except Exception:
            ollama_model = st.text_input("Model name", "llava:13b")
        temperature = st.slider("Temperature", 0.0, 1.0, 0.3, 0.05)


# â”€â”€ ë©”ì¸ ì½˜í…ì¸  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("LLM Image Analysis")

left_col, right_col = st.columns([1, 1])

# â”€â”€ ì™¼ìª½: ì´ë¯¸ì§€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with left_col:
    st.subheader("ì´ë¯¸ì§€")

    current_bytes = st.session_state.get("current_image_bytes")

    if current_bytes:
        st.image(current_bytes, caption="Viewerì—ì„œ ë¶ˆëŸ¬ì˜¨ ì´ë¯¸ì§€", use_column_width=True)
        if st.button("ì´ë¯¸ì§€ ì´ˆê¸°í™”", key="clear_img"):
            st.session_state.current_image_bytes = None
            st.rerun()
    else:
        st.info("Viewer í˜ì´ì§€ì—ì„œ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•˜ê±°ë‚˜, ì•„ë˜ì—ì„œ ì§ì ‘ ì—…ë¡œë“œí•˜ì„¸ìš”.")

    with st.expander("ì§ì ‘ ì—…ë¡œë“œ (DICOM / PNG / JPG)", expanded=not bool(current_bytes)):
        direct_upload = st.file_uploader(
            "íŒŒì¼ ì„ íƒ",
            type=["dcm", "png", "jpg", "jpeg"],
            key="llm_direct_upload",
        )
        if direct_upload is not None:
            try:
                if direct_upload.name.lower().endswith(".dcm"):
                    ds = load_xray(direct_upload.read())
                    pixel = extract_pixel_array(ds)
                    wc, ww = get_window_defaults(ds)
                    windowed = apply_windowing(pixel, wc, ww)
                    img_bytes = array_to_png_bytes(windowed)
                else:
                    img_bytes = direct_upload.read()

                st.session_state.current_image_bytes = img_bytes
                st.image(img_bytes, caption=direct_upload.name, use_column_width=True)
            except Exception as e:
                st.error(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")

# â”€â”€ ì˜¤ë¥¸ìª½: ë¶„ì„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with right_col:
    st.subheader("íŒë… ìš”ì²­")

    prompt_key = st.selectbox("í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿", list(PROMPT_TEMPLATES.keys()))
    prompt = st.text_area(
        "í”„ë¡¬í”„íŠ¸",
        value=PROMPT_TEMPLATES[prompt_key],
        height=200,
        key="llm_prompt_area",
    )

    can_analyze = availability[selected_llm] and bool(
        st.session_state.get("current_image_bytes")
    )

    analyze_btn = st.button(
        f"ğŸ” {selected_llm}ë¡œ íŒë… ìš”ì²­",
        type="primary",
        disabled=not can_analyze,
        use_container_width=True,
    )

    if not st.session_state.get("current_image_bytes"):
        st.warning("ì´ë¯¸ì§€ë¥¼ ë¨¼ì € ë¶ˆëŸ¬ì˜¤ì„¸ìš”.")
    elif not availability[selected_llm]:
        st.warning(f"{selected_llm}ê°€ ì‚¬ìš© ë¶ˆê°€ ìƒíƒœì…ë‹ˆë‹¤.")

    st.markdown("---")
    st.subheader("íŒë… ê²°ê³¼")

    report_placeholder = st.empty()

    # ì´ì „ íŒë…ë¬¸ í‘œì‹œ
    if st.session_state.get("last_report"):
        report_placeholder.markdown(st.session_state.last_report)

    if analyze_btn:
        image_bytes = st.session_state.current_image_bytes
        report_text = ""

        try:
            # í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            if selected_llm == "GPT":
                from llm.gpt_client import GPTClient
                client = GPTClient(
                    model=gpt_model,
                    temperature=temperature,
                    max_tokens=int(max_tokens),
                )
            elif selected_llm == "Gemini":
                from llm.gemini_client import GeminiClient
                client = GeminiClient(
                    model=gemini_model,
                    temperature=temperature,
                    max_tokens=int(max_tokens),
                )
            elif selected_llm == "MedGemma":
                from llm.medgemma_client import MedGemmaClient
                client = MedGemmaClient(
                    model=medgemma_model,
                    max_new_tokens=int(max_new_tokens),
                )
            elif selected_llm == "Ollama":
                from llm.ollama_client import OllamaClient
                client = OllamaClient(
                    model=ollama_model,
                    temperature=temperature,
                )

            # ìŠ¤íŠ¸ë¦¬ë° ë˜ëŠ” ë¸”ë¡œí‚¹ ë¶„ì„
            if client.supports_streaming:
                report_placeholder.markdown("ë¶„ì„ ì¤‘...")
                for chunk in client.stream_analyze(image_bytes, prompt):
                    report_text += chunk
                    report_placeholder.markdown(report_text + " â–Œ")
                report_placeholder.markdown(report_text)
            else:
                with st.spinner(f"{selected_llm} ë¶„ì„ ì¤‘..."):
                    report_text = client.analyze(image_bytes, prompt)
                report_placeholder.markdown(report_text)

            st.session_state.last_report = report_text

        except Exception as e:
            st.error(f"ë¶„ì„ ì‹¤íŒ¨: {e}")
            report_text = ""

    # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
    if st.session_state.get("last_report"):
        st.download_button(
            "ğŸ“„ íŒë…ë¬¸ ë‹¤ìš´ë¡œë“œ (.md)",
            data=st.session_state.last_report,
            file_name="radiology_report.md",
            mime="text/markdown",
            use_container_width=True,
        )
